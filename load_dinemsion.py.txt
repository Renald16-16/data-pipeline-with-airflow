from airflow.hooks.postgres_hook import PostgresHook
from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults
import logging

class LoadDimensionOperator(BaseOperator):
    '''
    Operator to load dim tables.
    '''
    ui_color = '#80BD9E'

    @apply_defaults
    def __init__(self,
                table='',
                redshift_conn_id='',
                sql_transform_command='',
                truncate=True,
                *args, **kwargs):
    '''
        Arg(s):
            table: which table to load to
            redshift_conn_id: redshift connection id (string)
            sql_transform_command: SQL command to transform the original data
            truncate: truncate table before loading or not
        '''
        super(LoadDimensionOperator, self).__init__(*args, **kwargs)
        self.table=table
        self.redshift_conn_id=redshift_conn_id
        self.sql_transform_command=sql_transform_command
        self.truncate=truncate
    
    def execute(self, context):
        '''
        truncate the dimension tables before loading the new data, then inserts the new data
        '''
        redshift_hook=PostgresHook(postgres_conn_id=self.redshift_conn_id)
        
        if self.truncate:
            logging.info(f"Truncating table {self.table}")
            redshift_hook.run('''
                BEGIN;
                  TRUNCATE TABLE {};
                COMMIT;
            ''').format(self.table)
            logging.info(f"Truncate of {self.table} successful")
            
        sql_command=('''
            BEGIN;
                INSERT INTO {} {}
                {};
            COMMIT;
        ''').format(self.table,self.sql_transform_command)   
            
    logging.info(f'Loading dim table: {self.table} ...')
    redshift_hook.run(sql_command)
    logging.info('Load successful')
    
